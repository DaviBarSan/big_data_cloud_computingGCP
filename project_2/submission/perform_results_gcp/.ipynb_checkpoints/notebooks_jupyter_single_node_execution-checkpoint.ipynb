{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a3a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, sum, row_number, lit\n",
    "from pyspark.sql.functions import radians, sin, cos, sqrt, atan2\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89984618-4b07-4477-aebf-6887c87a4673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.4.4\n",
      "numpy version: 1.22.4\n",
      "pyspark version: 3.3.2\n",
      "dask version: 2022.01.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    " \n",
    "print('pandas version: %s' % pd.__version__)\n",
    "print('numpy version: %s' % np.__version__)\n",
    "print('pyspark version: %s' % pyspark.__version__)\n",
    "import dask\n",
    "print('dask version: %s' % dask.__version__)\n",
    " \n",
    "import time\n",
    " \n",
    "def benchmark(f, df, benchmarks, name, **kwargs):\n",
    "    \"\"\"Benchmark the given function against the given DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f: function to benchmark\n",
    "    df: data frame\n",
    "    benchmarks: container for benchmark results\n",
    "    name: task name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Duration (in seconds) of the given operation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    ret = f(df, **kwargs)\n",
    "    benchmarks['duration'].append(time.time() - start_time)\n",
    "    benchmarks['task'].append(name)\n",
    "    print(f\"{name} took: {benchmarks['duration'][-1]} seconds\")\n",
    "    return benchmarks['duration'][-1]\n",
    " \n",
    "def get_results(benchmarks):\n",
    "    \"\"\"Return a pandas DataFrame containing benchmark results.\"\"\"\n",
    "    return pd.DataFrame.from_dict(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e23c35-a5ad-4e5d-b3a9-0ae1fe73108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>Int32</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>Int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>Int64</td>\n",
       "      <td>object</td>\n",
       "      <td>Int32</td>\n",
       "      <td>Int32</td>\n",
       "      <td>Int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              VendorID tpep_pickup_datetime tpep_dropoff_datetime passenger_count trip_distance RatecodeID store_and_fwd_flag PULocationID DOLocationID payment_type fare_amount    extra  mta_tax tip_amount tolls_amount improvement_surcharge total_amount congestion_surcharge Airport_fee\n",
       "npartitions=4                                                                                                                                                                                                                                                                                 \n",
       "                 Int32       datetime64[ns]        datetime64[ns]           Int64       float64      Int64             object        Int32        Int32        Int64     float64  float64  float64    float64      float64               float64      float64              float64     float64\n",
       "                   ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...\n",
       "                   ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...\n",
       "                   ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...\n",
       "                   ...                  ...                   ...             ...           ...        ...                ...          ...          ...          ...         ...      ...      ...        ...          ...                   ...          ...                  ...         ...\n",
       "Dask Name: read-parquet, 4 tasks"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_data = dd.read_parquet(\"gs://bucket-for-cluster-dataproc/data/*.parquet\")\n",
    " \n",
    "dask_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}\n",
    "dask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c0ea32f-a586-466b-9946-3150d087bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return dd.read_parquet(\"gs://bucket-for-cluster-dataproc/data/*.parquet\")\n",
    "  \n",
    "def count(df=None):\n",
    "    return len(df)\n",
    " \n",
    "def mean(df):\n",
    "    return df.fare_amount.mean().compute()\n",
    " \n",
    "def standard_deviation(df):\n",
    "    return df.fare_amount.std().compute()\n",
    " \n",
    "def mean_of_sum(df):\n",
    "    return (df.fare_amount + df.tip_amount).mean().compute()\n",
    " \n",
    "def sum_columns(df):\n",
    "    return (df.fare_amount + df.tip_amount).compute()\n",
    " \n",
    "def mean_of_product(df):\n",
    "    return (df.fare_amount * df.tip_amount).mean().compute()\n",
    " \n",
    "def product_columns(df):\n",
    "    return (df.fare_amount * df.tip_amount).compute()\n",
    "  \n",
    "def value_counts(df):\n",
    "    return df.fare_amount.value_counts().compute()\n",
    "  \n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.PULocationID\n",
    "    phi_1 = df.PULocationID\n",
    "    theta_2 = df.DOLocationID\n",
    "    phi_2 = df.DOLocationID\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.mean().compute()\n",
    "  \n",
    "def complicated_arithmetic_operation(df):\n",
    "    theta_1 = df.PULocationID\n",
    "    phi_1 = df.PULocationID\n",
    "    theta_2 = df.DOLocationID\n",
    "    phi_2 = df.DOLocationID\n",
    "    temp = (np.sin((theta_2-theta_1)/2*np.pi/180)**2\n",
    "           + np.cos(theta_1*np.pi/180)*np.cos(theta_2*np.pi/180) * np.sin((phi_2-phi_1)/2*np.pi/180)**2)\n",
    "    ret = 2 * np.arctan2(np.sqrt(temp), np.sqrt(1-temp))\n",
    "    return ret.compute()\n",
    "  \n",
    "def groupby_statistics(df):\n",
    "    return df.groupby(by='passenger_count').agg(\n",
    "      {\n",
    "        'fare_amount': ['mean', 'std'], \n",
    "        'tip_amount': ['mean', 'std']\n",
    "      }\n",
    "    ).compute()\n",
    "  \n",
    "other = groupby_statistics(dask_data)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    " \n",
    "def join_count(df, other):\n",
    "    return len(dd.merge(df, other, left_index=True, right_index=True))\n",
    " \n",
    "def join_data(df, other):\n",
    "    return dd.merge(df, other, left_index=True, right_index=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d6d8555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 0.1979684829711914 seconds\n",
      "count took: 1.1088001728057861 seconds\n",
      "mean took: 0.4638330936431885 seconds\n",
      "standard deviation took: 0.5320038795471191 seconds\n",
      "mean of columns addition took: 0.5899946689605713 seconds\n",
      "addition of columns took: 0.6646292209625244 seconds\n",
      "mean of columns multiplication took: 0.5967772006988525 seconds\n",
      "multiplication of columns took: 0.6533539295196533 seconds\n",
      "value counts took: 0.646568775177002 seconds\n",
      "mean of complex arithmetic ops took: 2.3988208770751953 seconds\n",
      "complex arithmetic ops took: 2.4099225997924805 seconds\n",
      "groupby statistics took: 8.0478835105896 seconds\n",
      "join count took: 9.842153549194336 seconds\n",
      "join took: 9.674828290939331 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.674828290939331"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=dask_benchmarks, name='read file')\n",
    "benchmark(count, df=dask_data, benchmarks=dask_benchmarks, name='count')\n",
    "benchmark(mean, df=dask_data, benchmarks=dask_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=dask_data, benchmarks=dask_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_data, benchmarks=dask_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=dask_data, benchmarks=dask_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_data, benchmarks=dask_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=dask_data, benchmarks=dask_benchmarks, name='value counts')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, df=dask_data, benchmarks=dask_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=dask_data, benchmarks=dask_benchmarks, name='groupby statistics')\n",
    "benchmark(join_count, dask_data, benchmarks=dask_benchmarks, name='join count', other=other)\n",
    "benchmark(join_data, dask_data, benchmarks=dask_benchmarks, name='join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa03dc49-d8ef-4f8e-bd16-9638b1517579",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (dask_data.tip_amount >= 1) & (dask_data.tip_amount <= 5)\n",
    " \n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    "  \n",
    "dask_filtered = filter_data(dask_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936256f",
   "metadata": {},
   "source": [
    "removed the count_index_lenght operation since the spark does not make use of these indexes in parquet files. So I have decided to remove it both from Dask and Pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c305efe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 8.17177677154541 seconds\n",
      "filtered mean took: 8.295343399047852 seconds\n",
      "filtered standard deviation took: 8.105826139450073 seconds\n",
      "filtered mean of columns addition took: 8.274187326431274 seconds\n",
      "filtered addition of columns took: 8.354636430740356 seconds\n",
      "filtered mean of columns multiplication took: 8.236481666564941 seconds\n",
      "filtered multiplication of columns took: 8.065267562866211 seconds\n",
      "filtered mean of complex arithmetic ops took: 8.513192653656006 seconds\n",
      "filtered complex arithmetic ops took: 8.402990579605103 seconds\n",
      "filtered value counts took: 8.176492691040039 seconds\n",
      "filtered groupby statistics took: 8.32035231590271 seconds\n",
      "filtered join count took: 9.742776870727539 seconds\n",
      "filtered join took: 9.847952127456665 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.847952127456665"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(count, dask_filtered, benchmarks=dask_benchmarks, name='filtered count')\n",
    "benchmark(mean, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, dask_filtered, benchmarks=dask_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, dask_filtered, benchmarks=dask_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=dask_filtered, benchmarks=dask_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, dask_filtered, benchmarks=dask_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, dask_filtered, benchmarks=dask_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, dask_filtered, benchmarks=dask_benchmarks, name='filtered groupby statistics')\n",
    " \n",
    "other = groupby_statistics(dask_filtered)\n",
    "other.columns = pd.Index([e[0]+'_' + e[1] for e in other.columns.tolist()])\n",
    " \n",
    "benchmark(join_count, dask_filtered, benchmarks=dask_benchmarks, name='filtered join count', other=other)\n",
    "benchmark(join_data, dask_filtered, benchmarks=dask_benchmarks, name='filtered join', other=other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286a1a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12931345"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_data = spark.read.parquet(\"gs://bucket-for-cluster-dataproc/data/*.parquet\")\n",
    "\n",
    "\n",
    "pyspark_benchmarks = {\n",
    "    'duration': [],  # in seconds\n",
    "    'task': [],\n",
    "}\n",
    "pyspark_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e0878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def read_file_parquet(df=None):\n",
    "    return spark.read.parquet(\"gs://bucket-for-cluster-dataproc/data/*.parquet\")\n",
    "  \n",
    "def count(df=None):\n",
    "    return df.count()\n",
    " \n",
    "\n",
    "def mean(df):\n",
    "    return df.agg(avg(df.fare_amount)).collect()\n",
    " \n",
    "def standard_deviation(df):\n",
    "    return df.agg(stddev(df.fare_amount)).collect()\n",
    " \n",
    "def mean_of_sum(df):\n",
    "    return df.select((col(\"fare_amount\") + col(\"tip_amount\")).alias(\"total_amount\")).agg(avg(\"total_amount\")).collect()\n",
    " \n",
    "def sum_columns(df):\n",
    "    result = df.select((col(\"fare_amount\") + col(\"tip_amount\")).alias(\"total_amount\")).collect()\n",
    "    return result\n",
    " \n",
    "def mean_of_product(df):\n",
    "    return df.select((col(\"fare_amount\") * col(\"tip_amount\")).alias(\"total_amount\")).agg(avg(\"total_amount\")).collect()\n",
    " \n",
    "def product_columns(df):\n",
    "    result = df.select((col(\"fare_amount\") * col(\"tip_amount\")).alias(\"total_prod\")).agg(avg(\"total_prod\")).collect()\n",
    "    return result\n",
    " \n",
    "def value_counts(df):\n",
    "    val_counts = df.groupBy(\"fare_amount\").count()\n",
    "    return val_counts.collect()\n",
    "\n",
    "def complicated_arithmetic_operation(df):\n",
    "    temp = (\n",
    "        (sin(radians(df['DOLocationID'] - df['PULocationID']) / 2) ** 2) +\n",
    "        (cos(radians(df['PULocationID'])) * cos(radians(df['DOLocationID'])) * (sin(radians(df['DOLocationID'] - df['PULocationID']) / 2) ** 2))\n",
    "    )\n",
    "    ret = 2 * atan2(sqrt(temp), sqrt(1 - temp))\n",
    "    return df.withColumn('result', ret).select('result').collect()\n",
    "\n",
    "def mean_of_complicated_arithmetic_operation(df):\n",
    "    temp = (\n",
    "        (sin(radians(df['DOLocationID'] - df['PULocationID']) / 2) ** 2) +\n",
    "        (cos(radians(df['PULocationID'])) * cos(radians(df['DOLocationID'])) * (sin(radians(df['DOLocationID'] - df['PULocationID']) / 2) ** 2))\n",
    "    )\n",
    "    ret = 2 * atan2(sqrt(temp), sqrt(1 - temp))\n",
    "    return df.withColumn('result', ret).agg(F.mean('result')).collect()[0][0]\n",
    "\n",
    "\n",
    "def groupby_statistics(df):\n",
    "    gb = df.groupBy('passenger_count').agg(\n",
    "        avg(\"fare_amount\"), stddev(\"fare_amount\"),\n",
    "        avg(\"tip_amount\"), stddev(\"tip_amount\")\n",
    "    )\n",
    "    return gb.toPandas()\n",
    "\n",
    "\n",
    "windowSpec = Window.orderBy(lit(1))\n",
    "other_spark = spark.createDataFrame(groupby_statistics(pyspark_data))\n",
    "other_spark = other_spark.withColumn(\"index\", row_number().over(windowSpec))\n",
    "#other_spark.columns = pd.Index([e[0]+'_' + e[1] for e in other_spark.columns.tolist()])\n",
    "pyspark_data_with_index = pyspark_data.withColumn(\"index\", row_number().over(windowSpec))\n",
    "\n",
    "def join_count(df, other):\n",
    "    joined_df = df.join(broadcast(other), on=\"index\")\n",
    "    # Count the number of rows in the joined DataFrame\n",
    "    count = joined_df.count()\n",
    "    return count\n",
    "\n",
    "def join_data(df, other):\n",
    "    # Use broadcast hint to optimize the join\n",
    "    ret = df.join(broadcast(other), on=\"index\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66658b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file took: 2.2425272464752197 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count took: 1.7583775520324707 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean took: 1.246199131011963 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard deviation took: 1.4145667552947998 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of columns addition took: 1.541130542755127 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition of columns took: 33.94906544685364 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:==============>                                           (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of columns multiplication took: 0.9470608234405518 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplication of columns took: 0.9508183002471924 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value counts took: 2.2307310104370117 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex arithmetic ops took: 30.635658025741577 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of complex arithmetic ops took: 1.8361237049102783 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby statistics took: 1.983147144317627 seconds\n",
      "join took: 0.047533273696899414 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 15:00:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:00:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join count took: 7.361225366592407 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.361225366592407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(read_file_parquet, df=None, benchmarks=pyspark_benchmarks, name='read file')\n",
    "benchmark(count, df=pyspark_data, benchmarks=pyspark_benchmarks, name='count')\n",
    "benchmark(mean, df=pyspark_data, benchmarks=pyspark_benchmarks, name='mean')\n",
    "benchmark(standard_deviation, df=pyspark_data, benchmarks=pyspark_benchmarks, name='standard deviation')\n",
    "benchmark(mean_of_sum, df=pyspark_data, benchmarks=pyspark_benchmarks, name='mean of columns addition')\n",
    "benchmark(sum_columns, df=pyspark_data, benchmarks=pyspark_benchmarks, name='addition of columns')\n",
    "benchmark(mean_of_product, df=pyspark_data, benchmarks=pyspark_benchmarks, name='mean of columns multiplication')\n",
    "benchmark(product_columns, df=pyspark_data, benchmarks=pyspark_benchmarks, name='multiplication of columns')\n",
    "benchmark(value_counts, df=pyspark_data, benchmarks=pyspark_benchmarks, name='value counts')\n",
    "benchmark(complicated_arithmetic_operation, df=pyspark_data, benchmarks=pyspark_benchmarks, name='complex arithmetic ops')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, df=pyspark_data, benchmarks=pyspark_benchmarks, name='mean of complex arithmetic ops')\n",
    "benchmark(groupby_statistics, df=pyspark_data, benchmarks=pyspark_benchmarks, name='groupby statistics')\n",
    "benchmark(join_data, pyspark_data_with_index, benchmarks=pyspark_benchmarks, name='join', other=other_spark)\n",
    "benchmark(join_count, pyspark_data_with_index, benchmarks=pyspark_benchmarks, name='join count', other=other_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b796183",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_filter = (pyspark_data.tip_amount >= 1) & (pyspark_data.tip_amount <= 5)\n",
    " \n",
    "def filter_data(df):\n",
    "    return df[expr_filter]\n",
    " \n",
    "pyspark_filtered = filter_data(pyspark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa5e38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1859063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered count took: 1.5542008876800537 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean took: 1.7325758934020996 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered standard deviation took: 1.5913488864898682 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean of columns addition took: 1.6500835418701172 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered addition of columns took: 15.418529272079468 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean of columns multiplication took: 1.5014209747314453 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered multiplication of columns took: 1.4281108379364014 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered mean of complex arithmetic ops took: 2.0476768016815186 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered complex arithmetic ops took: 18.683204174041748 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered value counts took: 2.228614568710327 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered groupby statistics took: 2.413958787918091 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 15:02:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered join took: 0.015772581100463867 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 15:02:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/30 15:02:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered join count took: 3.854327440261841 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.854327440261841"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark(count, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered count')\n",
    "benchmark(mean, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered mean')\n",
    "benchmark(standard_deviation, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered standard deviation')\n",
    "benchmark(mean_of_sum, pyspark_filtered, benchmarks=pyspark_benchmarks, name ='filtered mean of columns addition')\n",
    "benchmark(sum_columns, df=pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered addition of columns')\n",
    "benchmark(mean_of_product, pyspark_filtered, benchmarks=pyspark_benchmarks, name ='filtered mean of columns multiplication')\n",
    "benchmark(product_columns, df=pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered multiplication of columns')\n",
    "benchmark(mean_of_complicated_arithmetic_operation, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered mean of complex arithmetic ops')\n",
    "benchmark(complicated_arithmetic_operation, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered complex arithmetic ops')\n",
    "benchmark(value_counts, pyspark_filtered, benchmarks=pyspark_benchmarks, name ='filtered value counts')\n",
    "benchmark(groupby_statistics, pyspark_filtered, benchmarks=pyspark_benchmarks, name='filtered groupby statistics')\n",
    " \n",
    "other_spark = spark.createDataFrame(groupby_statistics(pyspark_filtered))\n",
    "other_spark = other_spark.withColumn(\"index\", row_number().over(windowSpec))\n",
    "#other_spark.columns = pd.Index([e[0]+'_' + e[1] for e in other_spark.columns.tolist()])\n",
    "pyspark_data_with_index_filtered = pyspark_filtered.withColumn(\"index\", row_number().over(windowSpec))\n",
    "    \n",
    "benchmark(join_data, pyspark_data_with_index_filtered, benchmarks=pyspark_benchmarks, name='filtered join', other=other_spark)\n",
    "benchmark(join_count, pyspark_data_with_index_filtered, benchmarks=pyspark_benchmarks, name='filtered join count', other=other_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74bfc72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>read file</th>\n",
       "      <td>0.187303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.196180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.271599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard deviation</th>\n",
       "      <td>0.353475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns addition</th>\n",
       "      <td>0.410502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addition of columns</th>\n",
       "      <td>7.157562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns multiplication</th>\n",
       "      <td>0.384705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiplication of columns</th>\n",
       "      <td>0.401257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value counts</th>\n",
       "      <td>0.554913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complex arithmetic ops</th>\n",
       "      <td>7.668461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of complex arithmetic ops</th>\n",
       "      <td>0.652696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupby statistics</th>\n",
       "      <td>0.673098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.011477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join count</th>\n",
       "      <td>1.799611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered count</th>\n",
       "      <td>0.459219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean</th>\n",
       "      <td>0.524072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered standard deviation</th>\n",
       "      <td>0.618865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns addition</th>\n",
       "      <td>0.502302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered addition of columns</th>\n",
       "      <td>4.067754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns multiplication</th>\n",
       "      <td>0.485611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered multiplication of columns</th>\n",
       "      <td>0.611489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of complex arithmetic ops</th>\n",
       "      <td>0.730174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered complex arithmetic ops</th>\n",
       "      <td>5.033169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered value counts</th>\n",
       "      <td>0.679700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered groupby statistics</th>\n",
       "      <td>0.707050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered join</th>\n",
       "      <td>0.011548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered join count</th>\n",
       "      <td>1.204182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         duration\n",
       "task                                             \n",
       "read file                                0.187303\n",
       "count                                    0.196180\n",
       "mean                                     0.271599\n",
       "standard deviation                       0.353475\n",
       "mean of columns addition                 0.410502\n",
       "addition of columns                      7.157562\n",
       "mean of columns multiplication           0.384705\n",
       "multiplication of columns                0.401257\n",
       "value counts                             0.554913\n",
       "complex arithmetic ops                   7.668461\n",
       "mean of complex arithmetic ops           0.652696\n",
       "groupby statistics                       0.673098\n",
       "join                                     0.011477\n",
       "join count                               1.799611\n",
       "filtered count                           0.459219\n",
       "filtered mean                            0.524072\n",
       "filtered standard deviation              0.618865\n",
       "filtered mean of columns addition        0.502302\n",
       "filtered addition of columns             4.067754\n",
       "filtered mean of columns multiplication  0.485611\n",
       "filtered multiplication of columns       0.611489\n",
       "filtered mean of complex arithmetic ops  0.730174\n",
       "filtered complex arithmetic ops          5.033169\n",
       "filtered value counts                    0.679700\n",
       "filtered groupby statistics              0.707050\n",
       "filtered join                            0.011548\n",
       "filtered join count                      1.204182"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_res_temp = get_results(pyspark_benchmarks).set_index('task')\n",
    "dask_res_temp = get_results(dask_benchmarks).set_index('task')\n",
    "pyspark_res_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c84d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pyspark</th>\n",
       "      <th>dask</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>read file</th>\n",
       "      <td>0.187303</td>\n",
       "      <td>0.136477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.196180</td>\n",
       "      <td>0.442867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.271599</td>\n",
       "      <td>0.312409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard deviation</th>\n",
       "      <td>0.353475</td>\n",
       "      <td>0.330494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns addition</th>\n",
       "      <td>0.410502</td>\n",
       "      <td>0.371634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>addition of columns</th>\n",
       "      <td>7.157562</td>\n",
       "      <td>0.352466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of columns multiplication</th>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.354498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiplication of columns</th>\n",
       "      <td>0.401257</td>\n",
       "      <td>0.353019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value counts</th>\n",
       "      <td>0.554913</td>\n",
       "      <td>0.369631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complex arithmetic ops</th>\n",
       "      <td>7.668461</td>\n",
       "      <td>0.881192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean of complex arithmetic ops</th>\n",
       "      <td>0.652696</td>\n",
       "      <td>0.893686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupby statistics</th>\n",
       "      <td>0.673098</td>\n",
       "      <td>2.634681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join</th>\n",
       "      <td>0.011477</td>\n",
       "      <td>2.859198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>join count</th>\n",
       "      <td>1.799611</td>\n",
       "      <td>2.830087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered count</th>\n",
       "      <td>0.459219</td>\n",
       "      <td>3.274809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean</th>\n",
       "      <td>0.524072</td>\n",
       "      <td>2.675765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered standard deviation</th>\n",
       "      <td>0.618865</td>\n",
       "      <td>2.698365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns addition</th>\n",
       "      <td>0.502302</td>\n",
       "      <td>2.696575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered addition of columns</th>\n",
       "      <td>4.067754</td>\n",
       "      <td>2.558876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of columns multiplication</th>\n",
       "      <td>0.485611</td>\n",
       "      <td>2.615428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered multiplication of columns</th>\n",
       "      <td>0.611489</td>\n",
       "      <td>2.645235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered mean of complex arithmetic ops</th>\n",
       "      <td>0.730174</td>\n",
       "      <td>2.854459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered complex arithmetic ops</th>\n",
       "      <td>5.033169</td>\n",
       "      <td>2.657660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered value counts</th>\n",
       "      <td>0.679700</td>\n",
       "      <td>2.683617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered groupby statistics</th>\n",
       "      <td>0.707050</td>\n",
       "      <td>2.803970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered join</th>\n",
       "      <td>0.011548</td>\n",
       "      <td>2.985797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filtered join count</th>\n",
       "      <td>1.204182</td>\n",
       "      <td>2.884312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          pyspark      dask\n",
       "task                                                       \n",
       "read file                                0.187303  0.136477\n",
       "count                                    0.196180  0.442867\n",
       "mean                                     0.271599  0.312409\n",
       "standard deviation                       0.353475  0.330494\n",
       "mean of columns addition                 0.410502  0.371634\n",
       "addition of columns                      7.157562  0.352466\n",
       "mean of columns multiplication           0.384705  0.354498\n",
       "multiplication of columns                0.401257  0.353019\n",
       "value counts                             0.554913  0.369631\n",
       "complex arithmetic ops                   7.668461  0.881192\n",
       "mean of complex arithmetic ops           0.652696  0.893686\n",
       "groupby statistics                       0.673098  2.634681\n",
       "join                                     0.011477  2.859198\n",
       "join count                               1.799611  2.830087\n",
       "filtered count                           0.459219  3.274809\n",
       "filtered mean                            0.524072  2.675765\n",
       "filtered standard deviation              0.618865  2.698365\n",
       "filtered mean of columns addition        0.502302  2.696575\n",
       "filtered addition of columns             4.067754  2.558876\n",
       "filtered mean of columns multiplication  0.485611  2.615428\n",
       "filtered multiplication of columns       0.611489  2.645235\n",
       "filtered mean of complex arithmetic ops  0.730174  2.854459\n",
       "filtered complex arithmetic ops          5.033169  2.657660\n",
       "filtered value counts                    0.679700  2.683617\n",
       "filtered groupby statistics              0.707050  2.803970\n",
       "filtered join                            0.011548  2.985797\n",
       "filtered join count                      1.204182  2.884312"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pyspark_res_temp.duration, dask_res_temp.duration],axis=1,keys=['pyspark', 'dask'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e4b4020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://bucket-for-cluster-dataproc/single_node_results_120930\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from os import getcwd\n",
    " \n",
    "filename = \"gs://bucket-for-cluster-dataproc/single_node_results_\" + datetime.now().strftime(\"%H%M%S\") *\"_4files\"\n",
    "print(filename)\n",
    " \n",
    "df.to_parquet(path=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}